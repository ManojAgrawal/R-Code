library(tokenizers)
library(lsa)
library(tm)
library(dplyr)
library(coreNLP)
library(NLP)
library(pROC)
library(randomForest)

rm(train)
#devtools::install_github("statsmaths/coreNLP")
#coreNLP::downloadCoreNLP()
initCoreNLP()

data <- train[5000:10000,]

# funtion to clean and create ngrams token
prep_text <- function(x) {
  x <- Corpus(VectorSource(x))
  
  x <- x %>%
    
    tm_map(content_transformer(tolower)) %>%
    tm_map(removePunctuation) %>%
#    tm_map(removeNumbers) %>%
    tm_map(removeWords, stopwords()) %>%
    tm_map(stemDocument) %>%
    tm_map(stripWhitespace)
  
  x <- x[[1]]$content
  x <-
    tokenize_ngrams(
      x,
      ngram_delim = " ",
      n = 3,
      n_min = 1,
      simplify = TRUE
    )
  
}


# funtion to annotate string

ann_text <- function(x) {
  obj <- annotateString(x)
  
  obj_NN <-
    obj$token$lemma[obj[1]$token$POS == "NN" |
                      obj[1]$token$POS == "NNS"]
  
  obj_NNP <-
    obj$token$lemma[obj[1]$token$POS == "NNP" |
                      obj[1]$token$POS == "NNPS"]
  
  # i checked with having all verb types but base verb and past verb gives the best results    
  obj_VB <-
    obj$token$lemma[obj[1]$token$POS == "VB" |
                      #                      obj[1]$token$POS == "VBP" |
                      obj[1]$token$POS == "VBD" ]
  #                      obj[1]$token$POS == "VBZ" |
  #                      obj[1]$token$POS == "VBN" |
  #                      obj[1]$token$POS == "VBG"]
  
  
  obj_WH <-
    obj$token$lemma[obj[1]$token$POS == "WDT" |
                      obj[1]$token$POS == "WP" |
                      obj[1]$token$POS == "WP$" |
                      obj[1]$token$POS == "WRB" ]
  
  result <- list(obj_NN, obj_NNP, obj_VB, obj_WH)
  
}

# loop on table and create similariy matrix
for (j in 1:nrow(data)) {
  
  # get n gram tokens for both questions
  que1_gms <- prep_text(data$question1[j])
  que1_gms <- unique(que1_gms)
  
  
  que2_gms <- prep_text(data$question2[j])
  que2_gms <- unique(que2_gms)
  
  # combine both vectors to get a combined vector   
  gms <- union(que1_gms, que2_gms)
  
  vect1 <- c()
  vect2 <- c()
  
  # if both vectors are not null, prepare vect1 and vect2 vectors and find similarity. If one vector is 
  # null and other is not define similarity as 0 and if both are null assume similarity as 0.5       
  if (length(que1_gms != 0) & length(que2_gms != 0)) {
    for (i in 1:length(gms)) {
      if (gms[i] %in% que1_gms) {
        vect1 <- append(vect1, 1)
      }
      
      else {
        vect1 <- append(vect1, 0)
      }
      if (gms[i] %in% que2_gms) {
        vect2 <- append(vect2, 1)
      }
      
      else {
        vect2 <- append(vect2, 0)
      }
    }
    
    data$sim_gms[j] <- cosine(vect1, vect2)
    
  } else if ((length(que1_gms == 0) &
              length(que2_gms != 0)) |
             (length(que1_gms != 0) & length(que2_gms == 0)))  {
    data$sim_gms[j] <- 0
    
  } else {
    data$sim_gms[j] <- 0.5
    
  }
  
  # get annotations and create vectors and find cosine similarity as grams vector above
  
  que1_ann <- ann_text(data$question1[j])
  
  que1_nn <- unique(que1_ann[[1]])
  que1_np <- unique(que1_ann[[2]])
  que1_vb <- unique(que1_ann[[3]])
  que1_wh <- unique(que1_ann[[4]])
  
  
  que2_ann <- ann_text(data$question2[j])
  
  que2_nn <- unique(que2_ann[[1]])
  que2_np <- unique(que2_ann[[2]])
  que2_vb <- unique(que2_ann[[3]])
  que2_wh <- unique(que2_ann[[4]])
  
  # similarity for common nouns
  
  nn <- union(que1_nn, que2_nn)
  
  vect1 <- c()
  vect2 <- c()
  
  if (length(que1_nn != 0) & length(que2_nn != 0)) {
    for (i in 1:length(nn)) {
      if (nn[i] %in% que1_nn) {
        vect1 <- append(vect1, 1)
      }
      
      else {
        vect1 <- append(vect1, 0)
      }
      if (nn[i] %in% que2_nn) {
        vect2 <- append(vect2, 1)
      }
      
      else {
        vect2 <- append(vect2, 0)
      }
    }
    
    data$sim_nn[j] <- cosine(vect1, vect2)
    
  } else if ((length(que1_nn == 0) &
              length(que2_nn != 0)) |
             (length(que1_nn != 0) & length(que2_nn == 0)))  {
    data$sim_nn[j] <- 0
    
  } else {
    data$sim_nn[j] <- 0.5
    
  }
  
  
  # similarity for proper nouns
  
  np <- union(que1_np, que2_np)
  
  vect1 <- c()
  vect2 <- c()
  
  if (length(que1_np != 0) & length(que2_np != 0)) {
    for (i in 1:length(np)) {
      if (np[i] %in% que1_np) {
        vect1 <- append(vect1, 1)
      }
      
      else {
        vect1 <- append(vect1, 0)
      }
      if (np[i] %in% que2_np) {
        vect2 <- append(vect2, 1)
      }
      
      else {
        vect2 <- append(vect2, 0)
      }
    }
    
    data$sim_np[j] <- cosine(vect1, vect2)
    
  }  else if ((length(que1_np == 0) &
               length(que2_np != 0)) |
              (length(que1_np != 0) & length(que2_np == 0)))  {
    data$sim_np[j] <- 0
    
  } else {
    data$sim_np[j] <- 0.5
    
  }
  
  # similarity for verbs
  
  vb <- union(que1_vb, que2_vb)
  
  vect1 <- c()
  vect2 <- c()
  
  if (length(que1_vb != 0) & length(que2_vb != 0)) {
    for (i in 1:length(vb)) {
      if (vb[i] %in% que1_vb) {
        vect1 <- append(vect1, 1)
      }
      
      else {
        vect1 <- append(vect1, 0)
      }
      if (vb[i] %in% que2_vb) {
        vect2 <- append(vect2, 1)
      }
      
      else {
        vect2 <- append(vect2, 0)
      }
    }
    
    data$sim_vb[j] <- cosine(vect1, vect2)
    
  }  else if ((length(que1_vb == 0) &
               length(que2_vb != 0)) |
              (length(que1_vb != 0) & length(que2_vb == 0)))  {
    data$sim_vb[j] <- 0
    
  } else {
    data$sim_vb[j] <- 0.5
    
  }
  
  # similarity for question terms
  
  wh <- union(que1_wh, que2_wh)
  
  vect1 <- c()
  vect2 <- c()
  
  if (length(que1_wh != 0) & length(que2_wh != 0)) {
    for (i in 1:length(wh)) {
      if (wh[i] %in% que1_wh) {
        vect1 <- append(vect1, 1)
      }
      
      else {
        vect1 <- append(vect1, 0)
      }
      if (wh[i] %in% que2_wh) {
        vect2 <- append(vect2, 1)
      }
      
      else {
        vect2 <- append(vect2, 0)
      }
    }
    
    data$sim_wh[j] <- cosine(vect1, vect2)
    
  }  else if ((length(que1_wh == 0) &
               length(que2_wh != 0)) |
              (length(que1_wh != 0) & length(que2_wh == 0)))  {
    data$sim_wh[j] <- 0
    
  } else {
    data$sim_wh[j] <- 0.5
    
  }  
  
}

# Logistic regression
model_log <- glm(is_duplicate~sim_gms+sim_np+sim_nn+sim_vb+sim_wh,data=data,family = quasibinomial(link = 'logit'))

summary(model_log)

data$prob_log <- predict(model,type=c("response"))

roc_log <- roc(data$is_duplicate,data$prob_log)
roc_log
# we get area under the curve 0.73, this is a fair model 

# random forest model


model_rf <- randomForest(is_duplicate~sim_gms+sim_np+sim_nn+sim_vb+sim_wh,data=data)

summary(model_rf)

data$prob_rf <- predict(model_rf,type=c("response"))

roc_rf <- roc(data$is_duplicate,data$prob_rf)
roc_rf

# we get approx 0.77 area under the curve which is fair to good


